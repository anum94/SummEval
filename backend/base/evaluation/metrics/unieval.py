import numpy as np
import torch
import torch.nn as nn
from nltk import sent_tokenize
from tqdm import tqdm
from transformers import AutoConfig, AutoModelForSeq2SeqLM, AutoTokenizer

from .metric import Metric

def get_device() -> str:
    # set the device
    if torch.cuda.is_available():
        print("CUDA AVAILABLE....")
        torch.cuda.empty_cache()
        return "cuda"
    else:
        return "cpu"
    
def add_question(dimension, output, src, ref=None):
    """
    Add questions to generate input in Bool-QA format for UniEval.

    dimension: specific dimension to be evaluated
    src: source input for different NLG tasks. For example, source document for summarization
         and dialogue history for dialogue response generation.
    output: output text generated by the models
    ref: human-annotataed groundtruth
    context: the context needed to evaluate several specific dimension. For example,
             additional factual information when evaluating engagingness and groundedness in dialogues.
    """

    input_with_question = []
    for i in range(len(output)):
        if dimension == "fluency":
            cur_input = (
                "question: Is this a fluent paragraph? </s> paragraph: " + output[i]
            )
        elif dimension == "coherence":
            cur_input = (
                "question: Is this a coherent summary to the document? </s> summary: "
                + output[i]
                + " </s> document: "
                + src[i]
            )
        elif dimension == "consistency":
            cur_input = (
                "question: Is this claim consistent with the document? </s> claim: "
                + output[i]
                + " </s> document: "
                + src[i]
            )
        elif dimension == "relevance":
            cur_input = (
                "question: Is this summary relevant to the reference? </s> summary: "
                + output[i]
                + " </s> reference: "
                + ref[i]
            )
        else:
            raise NotImplementedError(
                "The input format for this dimension is still undefined. Please customize it first."
            )

        input_with_question.append(cur_input)
    return input_with_question

def convert_to_json(
    output_list,
    src_list=None,
    ref_list=None,
    context_list=None,
    scores=None,
    doc_id=None,
    system_id=None,
):
    """
    Convert the data into the json format.

    output_list: a list of model output
    src_list: source input for different NLG tasks. For example, source document for summarization
              and dialogue history for dialogue response generation
    ref_list: human-annotated groundtruth
    context_list: the context needed to evaluate several specific dimension. For example,
                  additional factual information when evaluating engagingness and groundedness in dialogues
    scores: human scores for evaluating the model output. They can be used to calculate the correlation
            between evaluators and human judgements. The scores should be stored in a dictionary. For example,
            {'fluency': 2.0, 'coherence': 3.0} could be the human score for a sample.
    doc_id: the index of the input source. It can be used to calculate summary-level correlation for summarzation
    system_id: the index of the generation system. It can be used to calculate system-level correlation.
    """
    json_data = []
    for i in range(len(output_list)):
        cur = {}
        cur["system_output"] = output_list[i]
        if src_list is not None:
            cur["source"] = src_list[i]
        if ref_list is not None:
            cur["reference"] = ref_list[i]
        if context_list is not None:
            cur["context"] = context_list[i]
        if scores is not None:
            cur["scores"] = scores[i]
        if doc_id is not None:
            cur["doc_id"] = doc_id[i]
        if system_id is not None:
            cur["system_id"] = system_id[i]
        json_data.append(cur)
    return json_data

class UniEval(Metric):
    metric_name = "unieval"

    def compute(self, predictions: list, references: list, source: list) -> dict:
        data = convert_to_json(
            output_list=predictions, src_list=source, ref_list=references
        )
        # Initialize evaluator for a specific task
        evaluator = SumEvaluator(max_length=1024, device="cuda:0", cache_dir=None)
        # Get multi-dimensional evaluation scores
        eval_scores = evaluator.evaluate(data, print_result=True)
        return eval_scores


class UniEvaluator:
    def __init__(
        self, model_name_or_path, max_length=1024, device="cuda:0", cache_dir=None
    ):
        """Set up model"""
        self.device = device
        self.max_length = max_length

        self.config = AutoConfig.from_pretrained(
            model_name_or_path, cache_dir=cache_dir
        )
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name_or_path, cache_dir=cache_dir
        )
        self.model = AutoModelForSeq2SeqLM.from_pretrained(
            model_name_or_path, config=self.config, cache_dir=cache_dir
        )

        self.model.eval()
        self.model.to(device)

        self.softmax = nn.Softmax(dim=1)

        self.pos_id = self.tokenizer("Yes")["input_ids"][0]
        self.neg_id = self.tokenizer("No")["input_ids"][0]

    def score(self, inputs, batch_size=8):
        """
        Get scores for the given samples.
        final_score = postive_score / (postive_score + negative_score)
        """

        # The implementation of "forward" in T5 still requires decoder_input_ids.
        # Therefore, we construct a random one-word target sequence.
        # The content of the target has no effect on the final scores.
        tgts = ["No" for _ in range(len(inputs))]

        pos_score_list, neg_score_list = [], []
        for i in tqdm(range(0, len(inputs), batch_size)):
            src_list = inputs[i : i + batch_size]
            tgt_list = tgts[i : i + batch_size]
            try:
                with torch.no_grad():
                    encoded_src = self.tokenizer(
                        src_list,
                        max_length=self.max_length,
                        truncation=True,
                        padding=True,
                        return_tensors="pt",
                    )
                    encoded_tgt = self.tokenizer(
                        tgt_list,
                        max_length=self.max_length,
                        truncation=True,
                        padding=True,
                        return_tensors="pt",
                    )

                    src_tokens = encoded_src["input_ids"].to(self.device)
                    src_mask = encoded_src["attention_mask"].to(self.device)

                    tgt_tokens = (
                        encoded_tgt["input_ids"].to(self.device)[:, 0].unsqueeze(-1)
                    )

                    output = self.model(
                        input_ids=src_tokens, attention_mask=src_mask, labels=tgt_tokens
                    )
                    logits = output.logits.view(-1, self.model.config.vocab_size)

                    pos_score = self.softmax(logits)[:, self.pos_id]  # Yes
                    neg_score = self.softmax(logits)[:, self.neg_id]  # No

                    cur_pos_score = [x.item() for x in pos_score]
                    cur_neg_score = [x.item() for x in neg_score]
                    pos_score_list += cur_pos_score
                    neg_score_list += cur_neg_score

            except RuntimeError:
                print(f"source: {src_list}")
                print(f"target: {tgt_list}")
                exit(0)

        score_list = []
        for i in range(len(pos_score_list)):
            score_list.append(
                pos_score_list[i] / (pos_score_list[i] + neg_score_list[i])
            )

        return score_list


class SumEvaluator:
    def __init__(self, max_length=1024, device="cuda:0", cache_dir=None):
        """Set up evaluator for text summarization"""
        self.scorer = UniEvaluator(
            model_name_or_path="MingZhong/unieval-sum",
            max_length=max_length,
            device=get_device(),
            cache_dir=cache_dir,
        )
        self.task = "summarization"
        self.dimensions = ["coherence", "consistency", "fluency", "relevance"]

    def evaluate(self, data, dims=None, overall=True, print_result=False):
        """
        Get the scores of all the given dimensions

        dims: A list of dimensions to be evaluated. If dims is None, SumEvaluator will evaluate
              four dimensions: coherence, consistency, fluency, relevance.

        overall: indicates whether the overall score is to be calculated.
                 Overall score can be customized to a combination of scores based on different
                 dimensions. The default here is the average score of all the given dimensions.

        print_result: whether to print the average score of each dimension on the screen
        """
        n_data = len(data)
        eval_scores = [{} for _ in range(n_data)]

        if dims == None:
            eval_dims = self.dimensions
        else:
            assert isinstance(dims, list)
            eval_dims = dims

        for dim in eval_dims:
            print("Evaluating {} of {} samples !!!".format(dim, n_data))

            # Calculate average sentence-level scores for 'consistency' and 'fluency'
            if dim == "consistency" or dim == "fluency":
                src_list, output_list = [], []
                n_sents = []  # the number of sentences in each generated summary
                for i in range(n_data):
                    source = data[i]["source"]
                    system_outputs = sent_tokenize(data[i]["system_output"])
                    n_sents.append(len(system_outputs))
                    for j in range(len(system_outputs)):
                        src_list.append(source)
                        output_list.append(system_outputs[j])
                input_list = add_question(
                    dimension=dim, output=output_list, src=src_list
                )
                sent_score = self.scorer.score(input_list)

                # Get average score for each sample
                start_idx = 0
                score = []
                for cur_n_sent in n_sents:
                    score.append(
                        sum(sent_score[start_idx : start_idx + cur_n_sent]) / cur_n_sent
                    )
                    start_idx += cur_n_sent

            # Calculate summary-level score for 'coherence' and 'relevance'
            elif dim == "coherence" or dim == "relevance":
                src_list, output_list, ref_list = [], [], []
                for i in range(n_data):
                    src_list.append(data[i]["source"])
                    output_list.append(data[i]["system_output"])
                    if dim == "relevance":
                        ref_list.append(data[i]["reference"])
                input_list = add_question(
                    dimension=dim,
                    output=output_list,
                    src=src_list,
                    ref=ref_list,
                )
                score = self.scorer.score(input_list)

            # Please customize other dimensions here for summarization
            else:
                raise NotImplementedError(
                    "The input format for this dimension is still undefined. \
                                           Please customize it first."
                )

            for i in range(n_data):
                eval_scores[i][dim] = score[i]

        # Customize your overall score here.
        if overall == True:
            for i in range(n_data):
                eval_scores[i]["overall"] = np.mean(list(eval_scores[i].values()))

        return eval_scores
